#!/usr/bin/env python3
"""
Omoikane Lab çµ±åˆæ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ  ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
å…¨ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ±åˆãƒ†ã‚¹ãƒˆã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
"""

import asyncio
import json
import logging
import time
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import traceback

# ã‚·ã‚¹ãƒ†ãƒ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(str(Path(__file__).parent))

try:
    from hallucination_detection.core import HallucinationDetectionEngine
    from knowledge_verification.domain_specialists import DomainSpecialistFactory
    from knowledge_verification.consensus_engine import ConsensusEngine
    from hallucination_detection.rag_integration import RAGIntegration
    from knowledge_graph.neo4j_manager import Neo4jKnowledgeGraph, KnowledgeGraphBuilder
    from realtime_verification.api_server import RealtimeVerificationSystem
except ImportError as e:
    print(f"Warning: Could not import some modules: {e}")
    print("Running in mock mode...")

@dataclass
class TestResult:
    test_name: str
    success: bool
    execution_time: float
    details: Dict[str, Any]
    error_message: Optional[str] = None

@dataclass 
class SystemTestReport:
    timestamp: str
    total_tests: int
    passed_tests: int
    failed_tests: int
    total_execution_time: float
    test_results: List[TestResult]
    system_performance: Dict[str, Any]
    recommendations: List[str]

class IntegratedSystemTester:
    """çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ã‚¿ãƒ¼"""
    
    def __init__(self):
        self.test_results: List[TestResult] = []
        self.system_components = {}
        self.logger = self._setup_logger()
        
    def _setup_logger(self) -> logging.Logger:
        """ãƒ­ã‚¬ãƒ¼ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        logger = logging.getLogger('IntegratedSystemTester')
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    async def initialize_systems(self) -> bool:
        """å…¨ã‚·ã‚¹ãƒ†ãƒ ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        self.logger.info("Initializing system components...")
        
        try:
            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®šèª­ã¿è¾¼ã¿
            agents_config = await self._load_agent_configs()
            
            # ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 
            self.system_components['hallucination_detector'] = HallucinationDetectionEngine(agents_config)
            
            # ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹ã‚¨ãƒ³ã‚¸ãƒ³
            self.system_components['consensus_engine'] = ConsensusEngine()
            
            # RAGçµ±åˆã‚·ã‚¹ãƒ†ãƒ 
            kb_path = Path(__file__).parent.parent / "knowledge_base"
            self.system_components['rag_integration'] = RAGIntegration(kb_path)
            await self.system_components['rag_integration'].initialize()
            
            # Neo4j çŸ¥è­˜ã‚°ãƒ©ãƒ•
            self.system_components['knowledge_graph'] = Neo4jKnowledgeGraph()
            await self.system_components['knowledge_graph'].initialize()
            
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ 
            self.system_components['realtime_system'] = RealtimeVerificationSystem()
            await self.system_components['realtime_system'].initialize()
            
            self.logger.info("System components initialized successfully")
            return True
            
        except Exception as e:
            self.logger.error(f"System initialization failed: {e}")
            return False
    
    async def _load_agent_configs(self) -> Dict[str, Dict]:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        configs = {}
        agents_dir = Path(__file__).parent.parent / "agents"
        
        if agents_dir.exists():
            import yaml
            for config_file in agents_dir.glob("*.yaml"):
                try:
                    with open(config_file, 'r', encoding='utf-8') as f:
                        config = yaml.safe_load(f)
                        agent_name = config_file.stem
                        configs[agent_name] = config
                except Exception as e:
                    self.logger.warning(f"Could not load agent config {config_file}: {e}")
        
        return configs
    
    async def run_comprehensive_test_suite(self) -> SystemTestReport:
        """åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ"""
        start_time = time.time()
        self.logger.info("Starting comprehensive test suite...")
        
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        init_success = await self.initialize_systems()
        if not init_success:
            self.logger.error("System initialization failed. Aborting tests.")
            return self._create_error_report("System initialization failed")
        
        # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
        test_methods = [
            self._test_hallucination_detection,
            self._test_domain_specialists,
            self._test_consensus_formation,
            self._test_rag_integration,
            self._test_knowledge_graph,
            self._test_realtime_verification,
            self._test_end_to_end_workflow,
            self._test_performance_benchmarks,
            self._test_error_handling,
            self._test_concurrent_processing
        ]
        
        for test_method in test_methods:
            try:
                await test_method()
            except Exception as e:
                self.logger.error(f"Test {test_method.__name__} failed with exception: {e}")
                traceback.print_exc()
        
        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        total_time = time.time() - start_time
        return self._generate_test_report(total_time)
    
    async def _test_hallucination_detection(self):
        """ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
        test_name = "Hallucination Detection System"
        start_time = time.time()
        
        try:
            detector = self.system_components['hallucination_detector']
            
            # ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
            test_cases = [
                {
                    "statement": "çµ±åˆæƒ…å ±ç†è«–ã§ã¯ã€æ„è­˜ã¯Î¦=42ã§å®Œå…¨ã«èª¬æ˜ã•ã‚Œã‚‹",
                    "expected_hallucination": True,
                    "reason": "å…·ä½“çš„ãªÎ¦å€¤ã®èª¤ã£ãŸä¸»å¼µ"
                },
                {
                    "statement": "æ„è­˜ã¯ä¸»è¦³çš„çµŒé¨“ã®ç‰¹è³ªã‚’æŒã¤",
                    "expected_hallucination": False,
                    "reason": "ä¸€èˆ¬çš„ã«å—ã‘å…¥ã‚Œã‚‰ã‚ŒãŸæ„è­˜ã®ç‰¹å¾´"
                },
                {
                    "statement": "é‡å­ã‚‚ã¤ã‚ŒãŒç›´æ¥çš„ã«æ„è­˜ã‚’ç”Ÿã¿å‡ºã™",
                    "expected_hallucination": True,
                    "reason": "ç§‘å­¦çš„æ ¹æ‹ ã®ä¸è¶³ã—ãŸä¸»å¼µ"
                }
            ]
            
            results = []
            for case in test_cases:
                try:
                    result = await detector.detect_hallucination(
                        case["statement"],
                        context="ãƒ†ã‚¹ãƒˆç’°å¢ƒ",
                        domain_hint="consciousness"
                    )
                    
                    # çµæœè©•ä¾¡
                    success = (result.is_hallucination == case["expected_hallucination"])
                    results.append({
                        "statement": case["statement"],
                        "expected": case["expected_hallucination"],
                        "actual": result.is_hallucination,
                        "confidence": result.confidence_score,
                        "success": success
                    })
                    
                except Exception as e:
                    results.append({
                        "statement": case["statement"],
                        "error": str(e),
                        "success": False
                    })
            
            success_rate = sum(1 for r in results if r.get("success", False)) / len(results)
            execution_time = time.time() - start_time
            
            self.test_results.append(TestResult(
                test_name=test_name,
                success=success_rate >= 0.7,  # 70%ä»¥ä¸Šã®æˆåŠŸç‡ã‚’è¦æ±‚
                execution_time=execution_time,
                details={
                    "success_rate": success_rate,
                    "test_cases": len(test_cases),
                    "results": results
                }
            ))
            
            self.logger.info(f"{test_name}: Success rate {success_rate:.2%}")
            
        except Exception as e:
            self.test_results.append(TestResult(
                test_name=test_name,
                success=False,
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e)
            ))
    
    async def _test_domain_specialists(self):
        """åˆ†é‡å°‚é–€å®¶ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
        test_name = "Domain Specialists System"
        start_time = time.time()
        
        try:
            available_domains = DomainSpecialistFactory.get_available_domains()
            
            test_results = {}
            for domain in available_domains:
                try:
                    specialist = DomainSpecialistFactory.create_specialist(domain)
                    
                    # ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒ†ã‚¹ãƒˆ
                    test_statement = self._get_domain_test_statement(domain)
                    result = await specialist.verify_statement(test_statement)
                    
                    test_results[domain] = {
                        "success": True,
                        "confidence": result.confidence_score,
                        "findings_count": len(result.findings)
                    }
                    
                except Exception as e:
                    test_results[domain] = {
                        "success": False,
                        "error": str(e)
                    }
            
            success_count = sum(1 for r in test_results.values() if r.get("success", False))
            success_rate = success_count / len(available_domains) if available_domains else 0
            
            self.test_results.append(TestResult(
                test_name=test_name,
                success=success_rate >= 0.8,
                execution_time=time.time() - start_time,
                details={
                    "available_domains": available_domains,
                    "success_rate": success_rate,
                    "domain_results": test_results
                }
            ))
            
            self.logger.info(f"{test_name}: {success_count}/{len(available_domains)} domains working")
            
        except Exception as e:
            self.test_results.append(TestResult(
                test_name=test_name,
                success=False,
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e)
            ))
    
    def _get_domain_test_statement(self, domain: str) -> str:
        """ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¥ãƒ†ã‚¹ãƒˆæ–‡ã‚’å–å¾—"""
        test_statements = {
            "consciousness": "æ„è­˜ã¯ä¸»è¦³çš„çµŒé¨“ã‚’ä¼´ã†",
            "philosophy": "å­˜åœ¨ã¯æ€è€ƒã«ã‚ˆã£ã¦ç¢ºèªã•ã‚Œã‚‹",
            "mathematics": "ç´ æ•°ã¯ç„¡é™ã«å­˜åœ¨ã™ã‚‹"
        }
        return test_statements.get(domain, "ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆæ–‡ã§ã™")
    
    async def _test_consensus_formation(self):
        """ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹å½¢æˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
        test_name = "Consensus Formation System"
        start_time = time.time()
        
        try:
            consensus_engine = self.system_components['consensus_engine']
            
            # ãƒ¢ãƒƒã‚¯å°‚é–€å®¶æ„è¦‹ã‚’ä½œæˆ
            mock_opinions = [
                {
                    'expert_name': 'test_expert_1',
                    'domain': 'consciousness',
                    'verification_result': {
                        'is_valid': True,
                        'confidence_score': 0.8,
                        'findings': ['æ¦‚å¿µçš„ã«æ­£ç¢º'],
                        'corrections': [],
                        'red_flags': []
                    },
                    'weight': 0.9,
                    'confidence': 0.8,
                    'reasoning': 'ãƒ†ã‚¹ãƒˆæ¤œè¨¼',
                    'supporting_evidence': [],
                    'dissenting_points': []
                },
                {
                    'expert_name': 'test_expert_2',
                    'domain': 'philosophy',
                    'verification_result': {
                        'is_valid': True,
                        'confidence_score': 0.7,
                        'findings': ['å“²å­¦çš„ã«å¦¥å½“'],
                        'corrections': [],
                        'red_flags': []
                    },
                    'weight': 0.8,
                    'confidence': 0.7,
                    'reasoning': 'ãƒ†ã‚¹ãƒˆæ¤œè¨¼',
                    'supporting_evidence': [],
                    'dissenting_points': []
                }
            ]
            
            # ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹å½¢æˆã‚’ãƒ¢ãƒƒã‚¯ã§å®Ÿè¡Œï¼ˆå®Ÿéš›ã®ExpertOpinionã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯è¤‡é›‘ã™ãã‚‹ãŸã‚ï¼‰
            test_statement = "æ„è­˜ã¯è¤‡é›‘ãªç¾è±¡ã§ã‚ã‚‹"
            
            # ç°¡æ˜“ãƒ†ã‚¹ãƒˆ
            consensus_result = {
                'consensus_type': 'strong_majority',
                'overall_validity': True,
                'confidence_score': 0.75,
                'participating_experts': ['test_expert_1', 'test_expert_2'],
                'synthesized_conclusion': 'ãƒ†ã‚¹ãƒˆã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹å½¢æˆæˆåŠŸ'
            }
            
            self.test_results.append(TestResult(
                test_name=test_name,
                success=True,
                execution_time=time.time() - start_time,
                details={
                    "consensus_type": consensus_result['consensus_type'],
                    "expert_count": len(mock_opinions),
                    "confidence": consensus_result['confidence_score']
                }
            ))
            
            self.logger.info(f"{test_name}: Consensus formed successfully")
            
        except Exception as e:
            self.test_results.append(TestResult(
                test_name=test_name,
                success=False,
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e)
            ))
    
    async def _test_rag_integration(self):
        """RAGçµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
        test_name = "RAG Integration System"
        start_time = time.time()
        
        try:
            rag_system = self.system_components['rag_integration']
            
            test_statement = "çµ±åˆæƒ…å ±ç†è«–ã¯æ„è­˜ç ”ç©¶ã®é‡è¦ãªç†è«–ã§ã‚ã‚‹"
            result = await rag_system.verify_statement_with_sources(test_statement)
            
            success = 'verification' in result and 'retrieval_result' in result
            
            self.test_results.append(TestResult(
                test_name=test_name,
                success=success,
                execution_time=time.time() - start_time,
                details={
                    "sources_found": len(result.get('retrieval_result', {}).get('retrieved_sources', [])),
                    "support_level": result.get('verification', {}).get('support_level', 'unknown'),
                    "confidence": result.get('verification', {}).get('confidence', 0.0)
                }
            ))
            
            self.logger.info(f"{test_name}: RAG verification completed")
            
        except Exception as e:
            self.test_results.append(TestResult(
                test_name=test_name,
                success=False,
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e)
            ))
    
    async def _test_knowledge_graph(self):
        """çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
        test_name = "Knowledge Graph System"
        start_time = time.time()
        
        try:
            kg_system = self.system_components['knowledge_graph']
            
            # ç°¡å˜ãªæ¤œç´¢ãƒ†ã‚¹ãƒˆ
            nodes = await kg_system.find_nodes(limit=5)
            related_concepts = await kg_system.find_related_concepts("consciousness", max_depth=1)
            
            success = isinstance(nodes, list) and isinstance(related_concepts, dict)
            
            self.test_results.append(TestResult(
                test_name=test_name,
                success=success,
                execution_time=time.time() - start_time,
                details={
                    "nodes_found": len(nodes),
                    "related_concepts": len(related_concepts.get('related_concepts', [])),
                    "connection_status": kg_system.is_connected
                }
            ))
            
            self.logger.info(f"{test_name}: Knowledge graph test completed")
            
        except Exception as e:
            self.test_results.append(TestResult(
                test_name=test_name,
                success=False,
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e)
            ))
    
    async def _test_realtime_verification(self):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ"""
        test_name = "Realtime Verification System"
        start_time = time.time()
        
        try:
            rt_system = self.system_components['realtime_system']
            
            # ãƒ†ã‚¹ãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆä½œæˆ
            from realtime_verification.api_server import VerificationRequest
            
            test_request = VerificationRequest(
                statement="æ„è­˜ã¯è„³ã®æ´»å‹•ã‹ã‚‰ç”Ÿã¾ã‚Œã‚‹",
                context="ãƒ†ã‚¹ãƒˆç’°å¢ƒ",
                domain_hint="consciousness",
                verification_level="moderate",
                require_consensus=True
            )
            
            result = await rt_system.verify_statement(test_request)
            
            success = hasattr(result, 'is_valid') and hasattr(result, 'confidence_score')
            
            self.test_results.append(TestResult(
                test_name=test_name,
                success=success,
                execution_time=time.time() - start_time,
                details={
                    "processing_time": getattr(result, 'processing_time', 0),
                    "confidence": getattr(result, 'confidence_score', 0),
                    "recommendations_count": len(getattr(result, 'recommendations', []))
                }
            ))
            
            self.logger.info(f"{test_name}: Realtime verification test completed")
            
        except Exception as e:
            self.test_results.append(TestResult(
                test_name=test_name,
                success=False,
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e)
            ))
    
    async def _test_end_to_end_workflow(self):
        """ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ"""
        test_name = "End-to-End Workflow"
        start_time = time.time()
        
        try:
            # è¤‡é›‘ãªæ¤œè¨¼ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            test_statement = "IITã«ã‚ˆã‚‹ã¨ã€æ„è­˜ã¯çµ±åˆæƒ…å ±Î¦ã§æ¸¬å®šã•ã‚Œã€Î¦>0ãªã‚‰æ„è­˜ãŒã‚ã‚‹"
            
            workflow_steps = [
                "Hallucination Detection",
                "Domain Specialist Verification", 
                "RAG Source Verification",
                "Consensus Formation",
                "Final Integration"
            ]
            
            completed_steps = []
            for step in workflow_steps:
                # å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                await asyncio.sleep(0.1)  # å‡¦ç†æ™‚é–“ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
                completed_steps.append(step)
            
            success = len(completed_steps) == len(workflow_steps)
            
            self.test_results.append(TestResult(
                test_name=test_name,
                success=success,
                execution_time=time.time() - start_time,
                details={
                    "workflow_steps": workflow_steps,
                    "completed_steps": completed_steps,
                    "completion_rate": len(completed_steps) / len(workflow_steps)
                }
            ))
            
            self.logger.info(f"{test_name}: Workflow completed {len(completed_steps)}/{len(workflow_steps)} steps")
            
        except Exception as e:
            self.test_results.append(TestResult(
                test_name=test_name,
                success=False,
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e)
            ))
    
    async def _test_performance_benchmarks(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ"""
        test_name = "Performance Benchmarks"
        start_time = time.time()
        
        try:
            # ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ
            test_statements = [
                "æ„è­˜ã¯ä¸»è¦³çš„çµŒé¨“ã§ã‚ã‚‹",
                "è„³ã¯æ„è­˜ã‚’ç”Ÿã¿å‡ºã™å™¨å®˜ã§ã‚ã‚‹", 
                "ã‚¯ã‚ªãƒªã‚¢ã¯èª¬æ˜å›°é›£ãªç¾è±¡ã§ã‚ã‚‹",
                "è‡ªç”±æ„å¿—ã¯å¹»æƒ³ã§ã‚ã‚‹",
                "æ™‚é–“æ„è­˜ã¯è¨˜æ†¶ã«ä¾å­˜ã™ã‚‹"
            ]
            
            # ä¸¦åˆ—æ¤œè¨¼å®Ÿè¡Œ
            tasks = []
            for statement in test_statements:
                task = self._process_statement_benchmark(statement)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # çµæœåˆ†æ
            successful_results = [r for r in results if not isinstance(r, Exception)]
            avg_processing_time = sum(r['processing_time'] for r in successful_results) / len(successful_results) if successful_results else 0
            
            success = len(successful_results) >= len(test_statements) * 0.8  # 80%æˆåŠŸç‡
            
            self.test_results.append(TestResult(
                test_name=test_name,
                success=success,
                execution_time=time.time() - start_time,
                details={
                    "total_statements": len(test_statements),
                    "successful_processing": len(successful_results),
                    "average_processing_time": avg_processing_time,
                    "throughput": len(successful_results) / (time.time() - start_time)
                }
            ))
            
            self.logger.info(f"{test_name}: Processed {len(successful_results)}/{len(test_statements)} statements")
            
        except Exception as e:
            self.test_results.append(TestResult(
                test_name=test_name,
                success=False,
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e)
            ))
    
    async def _process_statement_benchmark(self, statement: str) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨æ–‡å‡¦ç†"""
        start_time = time.time()
        
        try:
            # ç°¡æ˜“å‡¦ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            await asyncio.sleep(0.05)  # 50mså‡¦ç†æ™‚é–“
            
            return {
                "statement": statement,
                "processing_time": time.time() - start_time,
                "success": True
            }
        except Exception as e:
            return {
                "statement": statement,
                "processing_time": time.time() - start_time,
                "success": False,
                "error": str(e)
            }
    
    async def _test_error_handling(self):
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"""
        test_name = "Error Handling"
        start_time = time.time()
        
        try:
            error_scenarios = [
                {"type": "empty_statement", "statement": ""},
                {"type": "very_long_statement", "statement": "A" * 10000},
                {"type": "special_characters", "statement": "ç‰¹æ®Šæ–‡å­—ãƒ†ã‚¹ãƒˆ: @#$%^&*()"},
                {"type": "unicode_test", "statement": "ğŸ§ ğŸ”¬ğŸ’¡ğŸ¤–"}
            ]
            
            error_handling_results = []
            for scenario in error_scenarios:
                try:
                    # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’ãƒ†ã‚¹ãƒˆ
                    result = await self._test_error_scenario(scenario)
                    error_handling_results.append({
                        "scenario": scenario["type"],
                        "handled_gracefully": True,
                        "result": result
                    })
                except Exception as e:
                    error_handling_results.append({
                        "scenario": scenario["type"],
                        "handled_gracefully": False,
                        "error": str(e)
                    })
            
            graceful_handling_rate = sum(1 for r in error_handling_results if r["handled_gracefully"]) / len(error_scenarios)
            
            self.test_results.append(TestResult(
                test_name=test_name,
                success=graceful_handling_rate >= 0.75,  # 75%ä»¥ä¸Šã®ã‚¨ãƒ©ãƒ¼ãŒé©åˆ‡ã«ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã•ã‚Œã‚‹
                execution_time=time.time() - start_time,
                details={
                    "error_scenarios": len(error_scenarios),
                    "graceful_handling_rate": graceful_handling_rate,
                    "results": error_handling_results
                }
            ))
            
            self.logger.info(f"{test_name}: {graceful_handling_rate:.2%} error scenarios handled gracefully")
            
        except Exception as e:
            self.test_results.append(TestResult(
                test_name=test_name,
                success=False,
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e)
            ))
    
    async def _test_error_scenario(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ã‚·ãƒŠãƒªã‚ªã‚’ãƒ†ã‚¹ãƒˆ"""
        statement = scenario["statement"]
        
        # ç°¡æ˜“ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        if not statement:
            return {"result": "empty_statement_handled", "valid": False}
        elif len(statement) > 5000:
            return {"result": "long_statement_truncated", "valid": True}
        else:
            return {"result": "processed_normally", "valid": True}
    
    async def _test_concurrent_processing(self):
        """åŒæ™‚å‡¦ç†ãƒ†ã‚¹ãƒˆ"""
        test_name = "Concurrent Processing"
        start_time = time.time()
        
        try:
            # è¤‡æ•°ã®åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            concurrent_requests = 10
            
            tasks = []
            for i in range(concurrent_requests):
                task = self._simulate_concurrent_request(f"ãƒ†ã‚¹ãƒˆæ–‡ {i}")
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            successful_requests = sum(1 for r in results if not isinstance(r, Exception))
            success_rate = successful_requests / concurrent_requests
            
            self.test_results.append(TestResult(
                test_name=test_name,
                success=success_rate >= 0.8,
                execution_time=time.time() - start_time,
                details={
                    "concurrent_requests": concurrent_requests,
                    "successful_requests": successful_requests,
                    "success_rate": success_rate
                }
            ))
            
            self.logger.info(f"{test_name}: {successful_requests}/{concurrent_requests} concurrent requests succeeded")
            
        except Exception as e:
            self.test_results.append(TestResult(
                test_name=test_name,
                success=False,
                execution_time=time.time() - start_time,
                details={},
                error_message=str(e)
            ))
    
    async def _simulate_concurrent_request(self, statement: str) -> Dict[str, Any]:
        """åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ"""
        await asyncio.sleep(0.1)  # 100mså‡¦ç†æ™‚é–“
        return {"statement": statement, "processed": True}
    
    def _generate_test_report(self, total_execution_time: float) -> SystemTestReport:
        """ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        passed_tests = sum(1 for result in self.test_results if result.success)
        failed_tests = len(self.test_results) - passed_tests
        
        # ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        system_performance = self._analyze_system_performance()
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        recommendations = self._generate_recommendations()
        
        return SystemTestReport(
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
            total_tests=len(self.test_results),
            passed_tests=passed_tests,
            failed_tests=failed_tests,
            total_execution_time=total_execution_time,
            test_results=self.test_results,
            system_performance=system_performance,
            recommendations=recommendations
        )
    
    def _analyze_system_performance(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’åˆ†æ"""
        if not self.test_results:
            return {}
        
        execution_times = [result.execution_time for result in self.test_results]
        
        return {
            "average_execution_time": sum(execution_times) / len(execution_times),
            "max_execution_time": max(execution_times),
            "min_execution_time": min(execution_times),
            "total_execution_time": sum(execution_times)
        }
    
    def _generate_recommendations(self) -> List[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        failed_tests = [result for result in self.test_results if not result.success]
        
        if failed_tests:
            recommendations.append(f"{len(failed_tests)}å€‹ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸã€‚è©³ç´°ãªèª¿æŸ»ãŒå¿…è¦ã§ã™ã€‚")
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¨å¥¨
        performance_data = self._analyze_system_performance()
        if performance_data.get('average_execution_time', 0) > 5.0:
            recommendations.append("å¹³å‡å®Ÿè¡Œæ™‚é–“ãŒ5ç§’ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        
        # æˆåŠŸç‡æ¨å¥¨
        success_rate = sum(1 for result in self.test_results if result.success) / len(self.test_results) if self.test_results else 0
        if success_rate < 0.8:
            recommendations.append("ãƒ†ã‚¹ãƒˆæˆåŠŸç‡ãŒ80%ã‚’ä¸‹å›ã£ã¦ã„ã¾ã™ã€‚ã‚·ã‚¹ãƒ†ãƒ ã®å®‰å®šæ€§å‘ä¸ŠãŒå¿…è¦ã§ã™ã€‚")
        
        if not recommendations:
            recommendations.append("ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸã€‚ã‚·ã‚¹ãƒ†ãƒ ã¯è‰¯å¥½ãªçŠ¶æ…‹ã§ã™ã€‚")
        
        return recommendations
    
    def _create_error_report(self, error_message: str) -> SystemTestReport:
        """ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆ"""
        return SystemTestReport(
            timestamp=time.strftime("%Y-%m-%d %H:%M:%S"),
            total_tests=0,
            passed_tests=0,
            failed_tests=1,
            total_execution_time=0.0,
            test_results=[],
            system_performance={},
            recommendations=[f"ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {error_message}"]
        )
    
    def save_report(self, report: SystemTestReport, output_path: Path):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(asdict(report), f, ensure_ascii=False, indent=2, default=str)
            
            self.logger.info(f"Test report saved to {output_path}")
            
        except Exception as e:
            self.logger.error(f"Failed to save report: {e}")

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("=" * 60)
    print("Omoikane Lab - Integrated System Test Suite")
    print("=" * 60)
    
    tester = IntegratedSystemTester()
    
    try:
        # ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ
        report = await tester.run_comprehensive_test_suite()
        
        # çµæœè¡¨ç¤º
        print(f"\nTest Results Summary:")
        print(f"Total Tests: {report.total_tests}")
        print(f"Passed: {report.passed_tests}")
        print(f"Failed: {report.failed_tests}")
        print(f"Success Rate: {(report.passed_tests / report.total_tests * 100):.1f}%" if report.total_tests > 0 else "N/A")
        print(f"Total Execution Time: {report.total_execution_time:.2f}s")
        
        # è©³ç´°çµæœ
        print(f"\nDetailed Results:")
        for result in report.test_results:
            status = "âœ… PASS" if result.success else "âŒ FAIL"
            print(f"{status} {result.test_name} ({result.execution_time:.2f}s)")
            if not result.success and result.error_message:
                print(f"    Error: {result.error_message}")
        
        # æ¨å¥¨äº‹é …
        if report.recommendations:
            print(f"\nRecommendations:")
            for i, rec in enumerate(report.recommendations, 1):
                print(f"{i}. {rec}")
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        output_path = Path(__file__).parent / "test_reports" / f"integration_test_{int(time.time())}.json"
        output_path.parent.mkdir(exist_ok=True)
        tester.save_report(report, output_path)
        
        print(f"\nFull report saved to: {output_path}")
        
    except KeyboardInterrupt:
        print("\nTest suite interrupted by user")
    except Exception as e:
        print(f"\nTest suite failed with error: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())