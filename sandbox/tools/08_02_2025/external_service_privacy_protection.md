# NewbornAI 2.0 外部サービスプライバシー保護仕様書

## 概要

外部サービス連携時の記憶データとプライバシー保護を重視した実装仕様です。ローカル環境での実用性を保ちながら、外部への情報漏洩を最小限に抑えます。

## プライバシー保護原則

### 1. データ最小化原則

```python
from typing import Dict, Any, List, Optional, Set\nfrom enum import Enum\nfrom dataclasses import dataclass\nimport hashlib\nimport json\nimport time\nfrom cryptography.fernet import Fernet\n\nclass PrivacyLevel(Enum):\n    \"\"\"プライバシーレベル\"\"\"\n    PUBLIC = \"public\"                 # 公開可能\n    PSEUDONYMIZED = \"pseudonymized\"   # 仮名化済み\n    AGGREGATED = \"aggregated\"         # 集約済み\n    ENCRYPTED = \"encrypted\"           # 暗号化\n    PROHIBITED = \"prohibited\"         # 共有禁止\n\nclass DataMinimizationProcessor:\n    \"\"\"データ最小化処理\"\"\"\n    \n    def __init__(self):\n        self.privacy_classifications = {\n            # 意識メトリクス\n            'phi_summary': PrivacyLevel.PUBLIC,\n            'stage_level': PrivacyLevel.PUBLIC,\n            'phi_trend': PrivacyLevel.PSEUDONYMIZED,\n            'complexity_metrics': PrivacyLevel.AGGREGATED,\n            \n            # 体験データ\n            'experience_categories': PrivacyLevel.AGGREGATED,\n            'sensory_patterns': PrivacyLevel.PSEUDONYMIZED,\n            'emotional_states': PrivacyLevel.PROHIBITED,\n            \n            # 記憶・時間意識\n            'memory_details': PrivacyLevel.PROHIBITED,\n            'temporal_patterns': PrivacyLevel.ENCRYPTED,\n            'personal_associations': PrivacyLevel.PROHIBITED,\n            \n            # 行動データ\n            'interaction_patterns': PrivacyLevel.AGGREGATED,\n            'preference_indicators': PrivacyLevel.PSEUDONYMIZED,\n            'usage_statistics': PrivacyLevel.AGGREGATED\n        }\n        \n    def apply_data_minimization(\n        self,\n        data: Dict[str, Any],\n        external_service: str,\n        purpose: str\n    ) -> Dict[str, Any]:\n        \"\"\"データ最小化適用\"\"\"\n        \n        minimized_data = {}\n        \n        for key, value in data.items():\n            privacy_level = self.privacy_classifications.get(key, PrivacyLevel.PROHIBITED)\n            \n            if privacy_level == PrivacyLevel.PUBLIC:\n                minimized_data[key] = value\n            elif privacy_level == PrivacyLevel.PSEUDONYMIZED:\n                minimized_data[key] = self.pseudonymize_data(value, external_service)\n            elif privacy_level == PrivacyLevel.AGGREGATED:\n                minimized_data[key] = self.aggregate_data(value, purpose)\n            elif privacy_level == PrivacyLevel.ENCRYPTED:\n                # 外部サービスには暗号化データとして送信（復号不可）\n                minimized_data[f\"{key}_encrypted\"] = self.create_privacy_placeholder(key)\n            # PROHIBITED データは除外\n            \n        return minimized_data\n        \n    def pseudonymize_data(self, data: Any, service_context: str) -> Any:\n        \"\"\"データ仮名化\"\"\"\n        \n        if isinstance(data, dict):\n            return {k: self.pseudonymize_data(v, service_context) for k, v in data.items()}\n        elif isinstance(data, list):\n            return [self.pseudonymize_data(item, service_context) for item in data]\n        elif isinstance(data, str) and len(data) > 3:\n            # 文字列データの仮名化\n            hash_input = f\"{service_context}:{data}\"\n            return f\"pseudo_{hashlib.sha256(hash_input.encode()).hexdigest()[:8]}\"\n        elif isinstance(data, (int, float)):\n            # 数値データの精度削減\n            if isinstance(data, float):\n                return round(data, 1)  # 小数点1桁まで\n            return data\n        else:\n            return data\n            \n    def aggregate_data(self, data: Any, purpose: str) -> Any:\n        \"\"\"データ集約\"\"\"\n        \n        if isinstance(data, list) and len(data) > 0:\n            if all(isinstance(x, (int, float)) for x in data):\n                # 数値リストの統計値\n                return {\n                    'count': len(data),\n                    'average': sum(data) / len(data),\n                    'range': [min(data), max(data)],\n                    'trend': 'increasing' if data[-1] > data[0] else 'decreasing' if data[-1] < data[0] else 'stable'\n                }\n            else:\n                # カテゴリカルデータの頻度\n                from collections import Counter\n                freq = Counter(data)\n                return {'categories': len(freq), 'most_common': freq.most_common(3)}\n        elif isinstance(data, dict):\n            # 辞書データの要約\n            return {\n                'keys_count': len(data),\n                'has_numeric_values': any(isinstance(v, (int, float)) for v in data.values()),\n                'complexity': 'high' if len(data) > 10 else 'medium' if len(data) > 3 else 'low'\n            }\n        else:\n            return {'type': type(data).__name__, 'available': True}\n            \n    def create_privacy_placeholder(self, original_key: str) -> Dict[str, Any]:\n        \"\"\"プライバシープレースホルダー作成\"\"\"\n        return {\n            'data_type': original_key,\n            'available_locally': True,\n            'privacy_protected': True,\n            'contact_for_access': 'Contact system administrator for data access'\n        }\n```\n\n### 2. 差分プライバシー実装\n\n```python\nimport numpy as np\nfrom typing import Callable, Union\n\nclass DifferentialPrivacyEngine:\n    \"\"\"差分プライバシーエンジン\"\"\"\n    \n    def __init__(self, epsilon: float = 1.0):\n        self.epsilon = epsilon  # プライバシー予算\n        self.used_budget = 0.0\n        \n    def add_laplace_noise(\n        self,\n        value: float,\n        sensitivity: float = 1.0,\n        epsilon_cost: float = 0.1\n    ) -> float:\n        \"\"\"ラプラスノイズ追加\"\"\"\n        \n        if self.used_budget + epsilon_cost > self.epsilon:\n            raise ValueError(\"Insufficient privacy budget\")\n            \n        scale = sensitivity / epsilon_cost\n        noise = np.random.laplace(0, scale)\n        \n        self.used_budget += epsilon_cost\n        return value + noise\n        \n    def add_gaussian_noise(\n        self,\n        value: float,\n        sensitivity: float = 1.0,\n        delta: float = 1e-5,\n        epsilon_cost: float = 0.1\n    ) -> float:\n        \"\"\"ガウシアンノイズ追加\"\"\"\n        \n        if self.used_budget + epsilon_cost > self.epsilon:\n            raise ValueError(\"Insufficient privacy budget\")\n            \n        sigma = sensitivity * np.sqrt(2 * np.log(1.25 / delta)) / epsilon_cost\n        noise = np.random.normal(0, sigma)\n        \n        self.used_budget += epsilon_cost\n        return value + noise\n        \n    def privatize_histogram(\n        self,\n        data: List[Any],\n        bins: int = 10,\n        epsilon_cost: float = 0.2\n    ) -> Dict[str, Any]:\n        \"\"\"ヒストグラムの差分プライバシー化\"\"\"\n        \n        if self.used_budget + epsilon_cost > self.epsilon:\n            return {'error': 'Insufficient privacy budget for histogram'}\n            \n        # 基本ヒストグラム作成\n        from collections import Counter\n        if all(isinstance(x, (int, float)) for x in data):\n            # 数値データ\n            hist, bin_edges = np.histogram(data, bins=bins)\n            noisy_hist = [max(0, int(self.add_laplace_noise(count, 1, epsilon_cost/bins))) \n                         for count in hist]\n            \n            self.used_budget += epsilon_cost\n            return {\n                'histogram': noisy_hist,\n                'bin_edges': bin_edges.tolist(),\n                'privacy_applied': True\n            }\n        else:\n            # カテゴリカルデータ\n            counter = Counter(data)\n            noisy_counts = {}\n            \n            budget_per_category = epsilon_cost / len(counter)\n            for category, count in counter.items():\n                noisy_counts[category] = max(0, int(\n                    self.add_laplace_noise(count, 1, budget_per_category)\n                ))\n                \n            self.used_budget += epsilon_cost\n            return {\n                'categories': noisy_counts,\n                'privacy_applied': True\n            }\n            \n    def get_remaining_budget(self) -> float:\n        \"\"\"残りプライバシー予算取得\"\"\"\n        return max(0, self.epsilon - self.used_budget)\n        \n    def reset_budget(self, new_epsilon: float = None):\n        \"\"\"プライバシー予算リセット\"\"\"\n        if new_epsilon is not None:\n            self.epsilon = new_epsilon\n        self.used_budget = 0.0\n```\n\n### 3. k-匿名性保証システム\n\n```python\nclass KAnonymityProcessor:\n    \"\"\"k-匿名性処理\"\"\"\n    \n    def __init__(self, k: int = 5):\n        self.k = k\n        \n    def ensure_k_anonymity(\n        self,\n        data_records: List[Dict[str, Any]],\n        quasi_identifiers: List[str]\n    ) -> List[Dict[str, Any]]:\n        \"\"\"k-匿名性保証\"\"\"\n        \n        if len(data_records) < self.k:\n            # レコード数が不足している場合は汎化\n            return self.generalize_insufficient_records(data_records, quasi_identifiers)\n            \n        # 準識別子による群分け\n        groups = self.group_by_quasi_identifiers(data_records, quasi_identifiers)\n        \n        anonymized_records = []\n        for group in groups:\n            if len(group) >= self.k:\n                # k-匿名性を満たしている群はそのまま\n                anonymized_records.extend(group)\n            else:\n                # k-匿名性を満たしていない群は汎化\n                generalized = self.generalize_group(group, quasi_identifiers)\n                anonymized_records.extend(generalized)\n                \n        return anonymized_records\n        \n    def group_by_quasi_identifiers(\n        self,\n        records: List[Dict[str, Any]],\n        quasi_identifiers: List[str]\n    ) -> List[List[Dict[str, Any]]]:\n        \"\"\"準識別子による群分け\"\"\"\n        \n        groups = {}\n        \n        for record in records:\n            # 準識別子の組み合わせをキーとする\n            key = tuple(record.get(qi, 'unknown') for qi in quasi_identifiers)\n            \n            if key not in groups:\n                groups[key] = []\n            groups[key].append(record)\n            \n        return list(groups.values())\n        \n    def generalize_group(\n        self,\n        group: List[Dict[str, Any]],\n        quasi_identifiers: List[str]\n    ) -> List[Dict[str, Any]]:\n        \"\"\"群の汎化\"\"\"\n        \n        if not group:\n            return group\n            \n        generalized_records = []\n        \n        for record in group:\n            generalized_record = record.copy()\n            \n            for qi in quasi_identifiers:\n                if qi in generalized_record:\n                    generalized_record[qi] = self.generalize_value(\n                        generalized_record[qi], qi\n                    )\n                    \n            generalized_records.append(generalized_record)\n            \n        return generalized_records\n        \n    def generalize_value(self, value: Any, attribute: str) -> str:\n        \"\"\"値の汎化\"\"\"\n        \n        if isinstance(value, (int, float)):\n            # 数値の範囲化\n            if attribute in ['phi_value', 'complexity']:\n                # φ値は大きな範囲に丸める\n                if value < 10:\n                    return '0-10'\n                elif value < 50:\n                    return '10-50'\n                elif value < 100:\n                    return '50-100'\n                else:\n                    return '100+'\n            else:\n                # その他の数値は10の位で丸める\n                rounded = round(value / 10) * 10\n                return f\"{rounded}-{rounded+9}\"\n        elif isinstance(value, str):\n            # 文字列の汎化\n            if len(value) > 10:\n                return f\"{value[:3]}***{value[-3:]}\"\n            else:\n                return \"generalized_string\"\n        else:\n            return \"generalized_value\"\n```\n\n### 4. 外部サービス別プライバシー設定\n\n```python\nclass ExternalServicePrivacyManager:\n    \"\"\"外部サービス別プライバシー管理\"\"\"\n    \n    def __init__(self):\n        self.service_privacy_policies = {\n            'anthropic_api': {\n                'allowed_data_types': ['phi_summary', 'stage_level'],\n                'privacy_level': PrivacyLevel.AGGREGATED,\n                'differential_privacy': True,\n                'epsilon': 0.5,\n                'k_anonymity': 5,\n                'max_data_age_hours': 24,\n                'user_consent_required': True\n            },\n            'creative_tools': {\n                'allowed_data_types': ['phi_summary', 'stage_level', 'visual_cues'],\n                'privacy_level': PrivacyLevel.PSEUDONYMIZED,\n                'differential_privacy': False,\n                'k_anonymity': 3,\n                'max_data_age_hours': 1,\n                'user_consent_required': True\n            },\n            'visualization_services': {\n                'allowed_data_types': ['phi_trends', 'stage_transitions', 'aggregated_patterns'],\n                'privacy_level': PrivacyLevel.AGGREGATED,\n                'differential_privacy': True,\n                'epsilon': 1.0,\n                'k_anonymity': 3,\n                'max_data_age_hours': None,\n                'user_consent_required': False\n            },\n            'unknown_external': {\n                'allowed_data_types': ['phi_summary'],\n                'privacy_level': PrivacyLevel.AGGREGATED,\n                'differential_privacy': True,\n                'epsilon': 0.1,  # 非常に厳しい制限\n                'k_anonymity': 10,\n                'max_data_age_hours': 1,\n                'user_consent_required': True\n            }\n        }\n        \n        # プライバシーエンジン初期化\n        self.dp_engines: Dict[str, DifferentialPrivacyEngine] = {}\n        self.k_anonymity_processor = KAnonymityProcessor()\n        self.data_minimizer = DataMinimizationProcessor()\n        \n    async def process_for_external_service(\n        self,\n        data: Dict[str, Any],\n        service_name: str,\n        service_category: str = 'unknown_external'\n    ) -> Dict[str, Any]:\n        \"\"\"外部サービス向けプライバシー保護処理\"\"\"\n        \n        # サービス固有のポリシー取得\n        policy = self.service_privacy_policies.get(\n            service_category, \n            self.service_privacy_policies['unknown_external']\n        )\n        \n        # データ最小化\n        minimized_data = self.data_minimizer.apply_data_minimization(\n            data, service_name, \"external_service_integration\"\n        )\n        \n        # 許可されたデータタイプのみフィルタリング\n        filtered_data = {\n            k: v for k, v in minimized_data.items()\n            if any(allowed in k for allowed in policy['allowed_data_types'])\n        }\n        \n        # 差分プライバシー適用\n        if policy['differential_privacy']:\n            filtered_data = await self.apply_differential_privacy(\n                filtered_data, service_name, policy['epsilon']\n            )\n            \n        # プライバシーメタデータ追加\n        filtered_data['_privacy_metadata'] = {\n            'privacy_level': policy['privacy_level'].value,\n            'differential_privacy_applied': policy['differential_privacy'],\n            'k_anonymity_level': policy['k_anonymity'],\n            'data_minimized': True,\n            'processing_timestamp': time.time()\n        }\n        \n        return filtered_data\n        \n    async def apply_differential_privacy(\n        self,\n        data: Dict[str, Any],\n        service_name: str,\n        epsilon: float\n    ) -> Dict[str, Any]:\n        \"\"\"差分プライバシー適用\"\"\"\n        \n        # サービス固有のDPエンジン取得\n        if service_name not in self.dp_engines:\n            self.dp_engines[service_name] = DifferentialPrivacyEngine(epsilon)\n            \n        dp_engine = self.dp_engines[service_name]\n        \n        if dp_engine.get_remaining_budget() < 0.1:\n            # 予算不足の場合はエラー\n            return {\n                'error': 'Privacy budget exhausted',\n                'retry_after_hours': 24\n            }\n            \n        private_data = {}\n        \n        for key, value in data.items():\n            if isinstance(value, (int, float)):\n                try:\n                    private_data[key] = dp_engine.add_laplace_noise(\n                        float(value), sensitivity=1.0, epsilon_cost=0.1\n                    )\n                except ValueError:\n                    # 予算不足の場合は元の値を汎化\n                    private_data[key] = self.fallback_generalization(value)\n            elif isinstance(value, list) and len(value) > 0:\n                try:\n                    private_data[key] = dp_engine.privatize_histogram(\n                        value, bins=5, epsilon_cost=0.2\n                    )\n                except ValueError:\n                    private_data[key] = {'count': len(value), 'privacy_protected': True}\n            else:\n                private_data[key] = value\n                \n        return private_data\n        \n    def fallback_generalization(self, value: Union[int, float]) -> str:\n        \"\"\"フォールバック汎化\"\"\"\n        if isinstance(value, float):\n            # 小数点以下切り捨てて範囲化\n            rounded = int(value)\n            return f\"{rounded//10*10}-{rounded//10*10+9}\"\n        else:\n            return f\"{value//10*10}-{value//10*10+9}\"\n```\n\n### 5. プライバシー監査システム\n\n```python\nclass PrivacyAuditLogger:\n    \"\"\"プライバシー監査ログ\"\"\"\n    \n    def __init__(self, log_file: str = \"./logs/privacy_audit.jsonl\"):\n        self.log_file = Path(log_file)\n        self.log_file.parent.mkdir(parents=True, exist_ok=True)\n        \n    async def log_data_sharing(\n        self,\n        service_name: str,\n        data_types_shared: List[str],\n        privacy_measures_applied: List[str],\n        user_consent: bool,\n        privacy_level: str\n    ):\n        \"\"\"データ共有ログ\"\"\"\n        \n        audit_entry = {\n            'timestamp': time.time(),\n            'event_type': 'data_sharing',\n            'service_name': service_name,\n            'data_types_shared': data_types_shared,\n            'privacy_measures': privacy_measures_applied,\n            'user_consent': user_consent,\n            'privacy_level': privacy_level,\n            'compliant': True\n        }\n        \n        await self.write_audit_entry(audit_entry)\n        \n    async def log_privacy_violation(\n        self,\n        violation_type: str,\n        service_name: str,\n        details: Dict[str, Any]\n    ):\n        \"\"\"プライバシー違反ログ\"\"\"\n        \n        violation_entry = {\n            'timestamp': time.time(),\n            'event_type': 'privacy_violation',\n            'violation_type': violation_type,\n            'service_name': service_name,\n            'details': details,\n            'severity': 'HIGH',\n            'action_required': True\n        }\n        \n        await self.write_audit_entry(violation_entry)\n        \n        # 即座にアラート\n        print(f\"[PRIVACY VIOLATION] {violation_type} with {service_name}: {details}\")\n        \n    async def write_audit_entry(self, entry: Dict[str, Any]):\n        \"\"\"監査エントリ書き込み\"\"\"\n        \n        try:\n            with open(self.log_file, 'a', encoding='utf-8') as f:\n                f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n        except Exception as e:\n            print(f\"Failed to write privacy audit log: {e}\")\n            \n    async def generate_privacy_report(self, days: int = 30) -> Dict[str, Any]:\n        \"\"\"プライバシーレポート生成\"\"\"\n        \n        cutoff_time = time.time() - (days * 24 * 3600)\n        \n        report = {\n            'total_data_sharing_events': 0,\n            'services_with_data_access': set(),\n            'privacy_violations': [],\n            'consent_rate': 0.0,\n            'most_shared_data_types': {},\n            'privacy_measure_usage': {}\n        }\n        \n        consent_granted = 0\n        total_sharing_events = 0\n        \n        try:\n            with open(self.log_file, 'r', encoding='utf-8') as f:\n                for line in f:\n                    try:\n                        entry = json.loads(line.strip())\n                        \n                        if entry['timestamp'] < cutoff_time:\n                            continue\n                            \n                        if entry['event_type'] == 'data_sharing':\n                            total_sharing_events += 1\n                            if entry['user_consent']:\n                                consent_granted += 1\n                                \n                            report['services_with_data_access'].add(entry['service_name'])\n                            \n                            # データタイプ統計\n                            for dt in entry['data_types_shared']:\n                                report['most_shared_data_types'][dt] = \\\n                                    report['most_shared_data_types'].get(dt, 0) + 1\n                                    \n                            # プライバシー対策統計\n                            for pm in entry['privacy_measures']:\n                                report['privacy_measure_usage'][pm] = \\\n                                    report['privacy_measure_usage'].get(pm, 0) + 1\n                                    \n                        elif entry['event_type'] == 'privacy_violation':\n                            report['privacy_violations'].append(entry)\n                            \n                    except json.JSONDecodeError:\n                        continue\n                        \n        except FileNotFoundError:\n            pass\n            \n        report['total_data_sharing_events'] = total_sharing_events\n        report['consent_rate'] = consent_granted / total_sharing_events if total_sharing_events > 0 else 0\n        report['services_with_data_access'] = list(report['services_with_data_access'])\n        \n        return report\n```\n\n### 6. 統合プライバシーフレームワーク\n\n```python\nclass ExternalServicePrivacyFramework:\n    \"\"\"外部サービスプライバシーフレームワーク統合\"\"\"\n    \n    def __init__(self):\n        self.privacy_manager = ExternalServicePrivacyManager()\n        self.audit_logger = PrivacyAuditLogger()\n        \n    async def secure_external_data_sharing(\n        self,\n        consciousness_data: Dict[str, Any],\n        service_name: str,\n        service_category: str,\n        purpose: str,\n        user_consent: bool = False\n    ) -> Dict[str, Any]:\n        \"\"\"セキュアな外部データ共有\"\"\"\n        \n        # ユーザー同意確認\n        if not user_consent and service_category != 'localhost':\n            await self.audit_logger.log_privacy_violation(\n                'missing_user_consent',\n                service_name,\n                {'purpose': purpose, 'attempted_data_types': list(consciousness_data.keys())}\n            )\n            return {'error': 'User consent required for external data sharing'}\n            \n        # プライバシー保護処理\n        try:\n            protected_data = await self.privacy_manager.process_for_external_service(\n                consciousness_data, service_name, service_category\n            )\n            \n            # 共有ログ記録\n            await self.audit_logger.log_data_sharing(\n                service_name,\n                list(protected_data.keys()),\n                self.extract_privacy_measures(protected_data),\n                user_consent,\n                protected_data.get('_privacy_metadata', {}).get('privacy_level', 'unknown')\n            )\n            \n            return protected_data\n            \n        except Exception as e:\n            await self.audit_logger.log_privacy_violation(\n                'processing_error',\n                service_name,\n                {'error': str(e), 'purpose': purpose}\n            )\n            raise\n            \n    def extract_privacy_measures(self, data: Dict[str, Any]) -> List[str]:\n        \"\"\"適用されたプライバシー対策の抽出\"\"\"\n        \n        measures = []\n        metadata = data.get('_privacy_metadata', {})\n        \n        if metadata.get('data_minimized'):\n            measures.append('data_minimization')\n        if metadata.get('differential_privacy_applied'):\n            measures.append('differential_privacy')\n        if metadata.get('k_anonymity_level', 0) > 1:\n            measures.append('k_anonymity')\n        if metadata.get('privacy_level') in ['pseudonymized', 'aggregated']:\n            measures.append(metadata['privacy_level'])\n            \n        return measures\n        \n    async def privacy_health_check(self) -> Dict[str, Any]:\n        \"\"\"プライバシーヘルスチェック\"\"\"\n        \n        report = await self.audit_logger.generate_privacy_report(days=7)\n        \n        health_status = {\n            'status': 'healthy',\n            'issues': [],\n            'recommendations': []\n        }\n        \n        # 同意率チェック\n        if report['consent_rate'] < 0.8:  # 80%未満\n            health_status['issues'].append(\n                f\"Low consent rate: {report['consent_rate']:.1%}\"\n            )\n            health_status['recommendations'].append(\n                \"Review consent request process and user communication\"\n            )\n            \n        # プライバシー違反チェック\n        if report['privacy_violations']:\n            health_status['status'] = 'critical'\n            health_status['issues'].append(\n                f\"{len(report['privacy_violations'])} privacy violations detected\"\n            )\n            health_status['recommendations'].append(\n                \"Immediate review of privacy violation incidents required\"\n            )\n            \n        # データ共有頻度チェック\n        if report['total_data_sharing_events'] > 1000:  # 週間1000回以上\n            health_status['issues'].append(\n                f\"High data sharing frequency: {report['total_data_sharing_events']} events in 7 days\"\n            )\n            health_status['recommendations'].append(\n                \"Consider implementing additional rate limiting\"\n            )\n            \n        return health_status\n```\n\n### 7. 使用例\n\n```python\n# プライバシーフレームワーク初期化\nprivacy_framework = ExternalServicePrivacyFramework()\n\n# 意識データの安全な外部共有例\nconsciousness_data = {\n    'phi_value': 25.7,\n    'development_stage': {'level': 3},\n    'personal_memories': ['private_experience'],\n    'temporal_consciousness': {'retention': ['memory1']}\n}\n\n# Anthropic API向け（厳格なプライバシー保護）\napi_data = await privacy_framework.secure_external_data_sharing(\n    consciousness_data,\n    'anthropic_api',\n    'external_api',\n    'Consciousness pattern analysis',\n    user_consent=True\n)\n\nprint(\"Data shared with Anthropic API:\")\nprint(json.dumps(api_data, indent=2))\n\n# プライバシーヘルスチェック\nhealth = await privacy_framework.privacy_health_check()\nprint(f\"\\nPrivacy status: {health['status']}\")\nif health['issues']:\n    print(\"Issues:\", health['issues'])\nif health['recommendations']:\n    print(\"Recommendations:\", health['recommendations'])\n```\n\nこの外部サービスプライバシー保護仕様により、記憶データの機密性を保ちながら、必要最小限の情報のみを外部サービスと安全に共有できます。