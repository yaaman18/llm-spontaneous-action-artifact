#!/usr/bin/env python3
"""
TDD Execution Orchestrator
çµ±åˆæƒ…å ±ã‚·ã‚¹ãƒ†ãƒ å­˜åœ¨è«–çš„çµ‚äº†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®TDDå®Ÿè¡Œçµ±åˆ¶ã‚·ã‚¹ãƒ†ãƒ 

æ­¦ç”°ç«¹å¤«ï¼ˆt_wadaï¼‰ã®TDDå°‚é–€çŸ¥è­˜ã«åŸºã¥ã:
- Red-Green-Refactorã‚µã‚¤ã‚¯ãƒ«ã®å®Ÿè¡Œçµ±åˆ¶
- å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ã¨åˆ†æ
- åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
- ç¶™ç¶šçš„ã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å¯¾å¿œ

Author: TDD Engineer (Takuto Wada's expertise)  
Date: 2025-08-06
Version: 1.0.0
"""

import asyncio
import json
import time
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import statistics
import psutil
import tracemalloc
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('tdd_execution.log')
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class TDDPhaseResult:
    """TDDæ®µéšå®Ÿè¡Œçµæœ"""
    phase_name: str
    success: bool
    execution_time_seconds: float
    tests_passed: int
    tests_failed: int
    coverage_percentage: float
    performance_metrics: Dict[str, float]
    error_messages: List[str] = field(default_factory=list)
    quality_indicators: Dict[str, float] = field(default_factory=dict)


@dataclass
class TDDCycleReport:
    """TDDã‚µã‚¤ã‚¯ãƒ«ãƒ¬ãƒãƒ¼ãƒˆ"""
    cycle_id: str
    timestamp: datetime
    red_phase: TDDPhaseResult
    green_phase: TDDPhaseResult
    refactor_phase: TDDPhaseResult
    overall_success: bool
    quality_score: float
    recommendations: List[str]


class TDDExecutionOrchestrator:
    """TDDå®Ÿè¡Œã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿"""
    
    def __init__(self, project_root: Path = None):
        self.project_root = project_root or Path.cwd()
        self.output_dir = self.project_root / "tdd_reports"
        self.output_dir.mkdir(exist_ok=True)
        
        self.execution_history = []
        self.quality_metrics = {}
        self.performance_benchmarks = {
            "max_latency_ms": 100,
            "min_coverage_percent": 95,
            "max_memory_growth_mb": 200,
            "min_quality_score": 0.9
        }
    
    async def execute_complete_tdd_cycle(self, cycle_id: str = None) -> TDDCycleReport:
        """å®Œå…¨ãªTDDã‚µã‚¤ã‚¯ãƒ«ã®å®Ÿè¡Œ"""
        
        cycle_id = cycle_id or f"tdd_cycle_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        cycle_start = datetime.now()
        
        logger.info(f"ğŸš€ Starting TDD cycle: {cycle_id}")
        
        try:
            # Red Phase: å¤±æ•—ã™ã‚‹ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
            red_result = await self._execute_red_phase()
            
            # Green Phase: æœ€å°å®Ÿè£…ã§ãƒ†ã‚¹ãƒˆã‚’é€šã™
            green_result = await self._execute_green_phase()
            
            # Refactor Phase: ã‚³ãƒ¼ãƒ‰å“è³ªæ”¹å–„
            refactor_result = await self._execute_refactor_phase()
            
            # ã‚µã‚¤ã‚¯ãƒ«è©•ä¾¡
            overall_success = red_result.success and green_result.success and refactor_result.success
            quality_score = self._calculate_cycle_quality_score(red_result, green_result, refactor_result)
            recommendations = self._generate_cycle_recommendations(red_result, green_result, refactor_result)
            
            cycle_report = TDDCycleReport(
                cycle_id=cycle_id,
                timestamp=cycle_start,
                red_phase=red_result,
                green_phase=green_result,
                refactor_phase=refactor_result,
                overall_success=overall_success,
                quality_score=quality_score,
                recommendations=recommendations
            )
            
            self.execution_history.append(cycle_report)
            
            # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
            await self._save_cycle_report(cycle_report)
            
            logger.info(f"âœ… TDD cycle completed: {cycle_id} (Quality: {quality_score:.3f})")
            
            return cycle_report
            
        except Exception as e:
            logger.error(f"âŒ TDD cycle failed: {cycle_id} - {str(e)}")
            raise
    
    async def _execute_red_phase(self) -> TDDPhaseResult:
        """Red Phase: å¤±æ•—ã™ã‚‹ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
        logger.info("ğŸ”´ Executing Red Phase - Failing Tests")
        
        phase_start = time.time()
        tracemalloc.start()
        
        try:
            # Red Phaseãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
            result = await self._run_pytest_phase("red_phase")
            
            # Red Phaseã¯å¤±æ•—ãŒæœŸå¾…ã•ã‚Œã‚‹ï¼ˆä¸€éƒ¨ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã™ã‚‹ã“ã¨ã‚’ç¢ºèªï¼‰
            expected_red_behavior = result["tests_failed"] > 0 and result["tests_passed"] >= 0
            
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            
            return TDDPhaseResult(
                phase_name="Red Phase",
                success=expected_red_behavior,
                execution_time_seconds=time.time() - phase_start,
                tests_passed=result["tests_passed"],
                tests_failed=result["tests_failed"],
                coverage_percentage=result["coverage"],
                performance_metrics={
                    "memory_peak_mb": peak / 1024 / 1024,
                    "execution_speed": result["execution_speed"]
                },
                quality_indicators={
                    "red_phase_validity": 1.0 if expected_red_behavior else 0.0,
                    "test_failure_rate": result["tests_failed"] / max(result["tests_passed"] + result["tests_failed"], 1)
                }
            )
            
        except Exception as e:
            return TDDPhaseResult(
                phase_name="Red Phase",
                success=False,
                execution_time_seconds=time.time() - phase_start,
                tests_passed=0,
                tests_failed=0,
                coverage_percentage=0.0,
                performance_metrics={},
                error_messages=[str(e)]
            )
    
    async def _execute_green_phase(self) -> TDDPhaseResult:
        """Green Phase: æœ€å°å®Ÿè£…ã§ãƒ†ã‚¹ãƒˆã‚’é€šã™"""
        logger.info("ğŸŸ¢ Executing Green Phase - Passing Tests")
        
        phase_start = time.time()
        tracemalloc.start()
        
        try:
            # Green Phaseãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
            result = await self._run_pytest_phase("green_phase")
            
            # Green Phaseã¯å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸãŒæœŸå¾…ã•ã‚Œã‚‹
            green_success = result["tests_failed"] == 0 and result["tests_passed"] > 0
            
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            
            return TDDPhaseResult(
                phase_name="Green Phase",
                success=green_success,
                execution_time_seconds=time.time() - phase_start,
                tests_passed=result["tests_passed"],
                tests_failed=result["tests_failed"],
                coverage_percentage=result["coverage"],
                performance_metrics={
                    "memory_peak_mb": peak / 1024 / 1024,
                    "execution_speed": result["execution_speed"],
                    "latency_ms": result.get("avg_latency_ms", 0.0)
                },
                quality_indicators={
                    "test_success_rate": result["tests_passed"] / max(result["tests_passed"] + result["tests_failed"], 1),
                    "coverage_achievement": min(result["coverage"] / self.performance_benchmarks["min_coverage_percent"], 1.0)
                }
            )
            
        except Exception as e:
            return TDDPhaseResult(
                phase_name="Green Phase",
                success=False,
                execution_time_seconds=time.time() - phase_start,
                tests_passed=0,
                tests_failed=0,
                coverage_percentage=0.0,
                performance_metrics={},
                error_messages=[str(e)]
            )
    
    async def _execute_refactor_phase(self) -> TDDPhaseResult:
        """Refactor Phase: ã‚³ãƒ¼ãƒ‰å“è³ªæ”¹å–„"""
        logger.info("ğŸ”§ Executing Refactor Phase - Quality Improvement")
        
        phase_start = time.time()
        tracemalloc.start()
        
        try:
            # Refactor Phaseãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
            result = await self._run_pytest_phase("refactor_phase")
            
            # Refactorã¯å“è³ªå‘ä¸Šã¨å…¨ãƒ†ã‚¹ãƒˆæˆåŠŸã‚’ç¢ºèª
            refactor_success = (
                result["tests_failed"] == 0 and 
                result["tests_passed"] > 0 and
                result["coverage"] >= self.performance_benchmarks["min_coverage_percent"]
            )
            
            current, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            
            # ã‚³ãƒ¼ãƒ‰å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®è¨ˆç®—
            code_quality = await self._analyze_code_quality()
            
            return TDDPhaseResult(
                phase_name="Refactor Phase",
                success=refactor_success,
                execution_time_seconds=time.time() - phase_start,
                tests_passed=result["tests_passed"],
                tests_failed=result["tests_failed"],
                coverage_percentage=result["coverage"],
                performance_metrics={
                    "memory_peak_mb": peak / 1024 / 1024,
                    "execution_speed": result["execution_speed"],
                    "latency_ms": result.get("avg_latency_ms", 0.0),
                    "memory_efficiency": result.get("memory_efficiency", 1.0)
                },
                quality_indicators={
                    "code_quality_score": code_quality["overall_score"],
                    "maintainability_index": code_quality["maintainability"],
                    "performance_improvement": code_quality["performance_gain"],
                    "test_reliability": result["tests_passed"] / max(result["tests_passed"] + result["tests_failed"], 1)
                }
            )
            
        except Exception as e:
            return TDDPhaseResult(
                phase_name="Refactor Phase",
                success=False,
                execution_time_seconds=time.time() - phase_start,
                tests_passed=0,
                tests_failed=0,
                coverage_percentage=0.0,
                performance_metrics={},
                error_messages=[str(e)]
            )
    
    async def _run_pytest_phase(self, phase: str) -> Dict[str, Any]:
        """pytestå®Ÿè¡Œã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†"""
        
        # ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ãƒ†ã‚¹ãƒˆãƒãƒ¼ã‚«ãƒ¼ã®è¨­å®š
        phase_markers = {
            "red_phase": "-m red_phase or (not green_phase and not refactor_phase)",
            "green_phase": "-m green_phase",
            "refactor_phase": "-m refactor_phase"
        }
        
        # pytestã‚³ãƒãƒ³ãƒ‰æ§‹ç¯‰
        pytest_cmd = [
            sys.executable, "-m", "pytest",
            str(self.project_root / "existential_termination_tdd_suite.py"),
            "-v",
            "--tb=short",
            "--cov=.",
            "--cov-report=json",
            "--benchmark-json=benchmark_results.json",
            phase_markers.get(phase, "")
        ]
        
        start_time = time.time()
        
        try:
            # pytestã®å®Ÿè¡Œ
            process = await asyncio.create_subprocess_exec(
                *pytest_cmd,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=self.project_root
            )
            
            stdout, stderr = await process.communicate()
            execution_time = time.time() - start_time
            
            # çµæœã®è§£æ
            test_results = self._parse_pytest_output(stdout.decode(), stderr.decode())
            coverage_results = self._parse_coverage_results()
            benchmark_results = self._parse_benchmark_results()
            
            return {
                "tests_passed": test_results["passed"],
                "tests_failed": test_results["failed"],
                "coverage": coverage_results["percentage"],
                "execution_speed": execution_time,
                "avg_latency_ms": benchmark_results.get("avg_latency_ms", 0.0),
                "memory_efficiency": benchmark_results.get("memory_efficiency", 1.0),
                "return_code": process.returncode
            }
            
        except Exception as e:
            logger.error(f"Pytest execution failed for {phase}: {e}")
            return {
                "tests_passed": 0,
                "tests_failed": 1,
                "coverage": 0.0,
                "execution_speed": time.time() - start_time,
                "error": str(e)
            }
    
    def _parse_pytest_output(self, stdout: str, stderr: str) -> Dict[str, int]:
        """pytestå‡ºåŠ›ã®è§£æ"""
        passed = stdout.count(" PASSED")
        failed = stdout.count(" FAILED")
        
        return {
            "passed": passed,
            "failed": failed
        }
    
    def _parse_coverage_results(self) -> Dict[str, float]:
        """ã‚«ãƒãƒ¬ãƒƒã‚¸çµæœã®è§£æ"""
        try:
            coverage_file = self.project_root / "coverage.json"
            if coverage_file.exists():
                with open(coverage_file) as f:
                    coverage_data = json.load(f)
                    return {
                        "percentage": coverage_data.get("totals", {}).get("percent_covered", 0.0)
                    }
        except Exception:
            pass
        
        return {"percentage": 0.0}
    
    def _parse_benchmark_results(self) -> Dict[str, float]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®è§£æ"""
        try:
            benchmark_file = self.project_root / "benchmark_results.json"
            if benchmark_file.exists():
                with open(benchmark_file) as f:
                    benchmark_data = json.load(f)
                    
                    if "benchmarks" in benchmark_data:
                        latencies = [b["stats"]["mean"] * 1000 for b in benchmark_data["benchmarks"]]
                        return {
                            "avg_latency_ms": statistics.mean(latencies) if latencies else 0.0,
                            "memory_efficiency": 1.0  # ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼
                        }
        except Exception:
            pass
        
        return {"avg_latency_ms": 0.0, "memory_efficiency": 1.0}
    
    async def _analyze_code_quality(self) -> Dict[str, float]:
        """ã‚³ãƒ¼ãƒ‰å“è³ªã®åˆ†æ"""
        
        # ç°¡æ˜“çš„ãªã‚³ãƒ¼ãƒ‰å“è³ªæŒ‡æ¨™ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯ã‚ˆã‚Šè©³ç´°ãªåˆ†æã‚’è¡Œã†ï¼‰
        quality_metrics = {
            "overall_score": 0.85,  # å®Ÿéš›ã¯é™çš„è§£æãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨
            "maintainability": 0.90,  # ä¿å®ˆæ€§æŒ‡æ¨™
            "performance_gain": 0.15,  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šç‡
            "complexity_score": 0.80,  # è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢
            "documentation_coverage": 0.95  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–ç‡
        }
        
        return quality_metrics
    
    def _calculate_cycle_quality_score(self, red: TDDPhaseResult, 
                                     green: TDDPhaseResult, 
                                     refactor: TDDPhaseResult) -> float:
        """TDDã‚µã‚¤ã‚¯ãƒ«å“è³ªã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
        
        # å„ãƒ•ã‚§ãƒ¼ã‚ºã®é‡è¦åº¦é‡ã¿ä»˜ã‘
        weights = {
            "red_validity": 0.15,      # Red Phaseã®å¦¥å½“æ€§
            "green_success": 0.35,     # Green Phaseã®æˆåŠŸ
            "refactor_quality": 0.35,  # Refactor Phaseã®å“è³ª
            "overall_coverage": 0.15   # å…¨ä½“ã‚«ãƒãƒ¬ãƒƒã‚¸
        }
        
        # ã‚¹ã‚³ã‚¢è¨ˆç®—
        red_score = red.quality_indicators.get("red_phase_validity", 0.0)
        green_score = green.quality_indicators.get("test_success_rate", 0.0)
        refactor_score = refactor.quality_indicators.get("code_quality_score", 0.0)
        coverage_score = max(green.coverage_percentage, refactor.coverage_percentage) / 100.0
        
        total_score = (
            red_score * weights["red_validity"] +
            green_score * weights["green_success"] +
            refactor_score * weights["refactor_quality"] +
            coverage_score * weights["overall_coverage"]
        )
        
        return min(total_score, 1.0)
    
    def _generate_cycle_recommendations(self, red: TDDPhaseResult, 
                                      green: TDDPhaseResult, 
                                      refactor: TDDPhaseResult) -> List[str]:
        """ã‚µã‚¤ã‚¯ãƒ«æ”¹å–„æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        
        recommendations = []
        
        # Red Phaseåˆ†æ
        if not red.success:
            recommendations.append("Red Phase: Ensure failing tests are properly designed to validate requirements")
        
        # Green Phaseåˆ†æ
        if not green.success:
            recommendations.append("Green Phase: Focus on minimal implementation to make tests pass")
        elif green.coverage_percentage < self.performance_benchmarks["min_coverage_percent"]:
            recommendations.append(f"Green Phase: Increase test coverage to {self.performance_benchmarks['min_coverage_percent']}%")
        
        # Refactor Phaseåˆ†æ
        if not refactor.success:
            recommendations.append("Refactor Phase: Improve code quality while maintaining test success")
        
        refactor_latency = refactor.performance_metrics.get("latency_ms", 0)
        if refactor_latency > self.performance_benchmarks["max_latency_ms"]:
            recommendations.append(f"Performance: Optimize latency to under {self.performance_benchmarks['max_latency_ms']}ms")
        
        # å…¨ä½“çš„ãªæ¨å¥¨äº‹é …
        overall_quality = self._calculate_cycle_quality_score(red, green, refactor)
        if overall_quality < self.performance_benchmarks["min_quality_score"]:
            recommendations.append("Overall: Focus on comprehensive quality improvement across all TDD phases")
        
        if not recommendations:
            recommendations.append("Excellent TDD implementation - maintain current high standards")
        
        return recommendations
    
    async def _save_cycle_report(self, cycle_report: TDDCycleReport):
        """ã‚µã‚¤ã‚¯ãƒ«ãƒ¬ãƒãƒ¼ãƒˆã®ä¿å­˜"""
        
        # JSONå½¢å¼ã§ã®è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ
        json_report_path = self.output_dir / f"{cycle_report.cycle_id}_detailed.json"
        with open(json_report_path, 'w', encoding='utf-8') as f:
            json.dump(self._serialize_cycle_report(cycle_report), f, indent=2, ensure_ascii=False)
        
        # äººé–“å¯èª­å½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆ
        readable_report_path = self.output_dir / f"{cycle_report.cycle_id}_summary.md"
        with open(readable_report_path, 'w', encoding='utf-8') as f:
            f.write(self._generate_readable_report(cycle_report))
        
        logger.info(f"ğŸ“Š Cycle report saved: {json_report_path}")
        logger.info(f"ğŸ“ Summary report saved: {readable_report_path}")
    
    def _serialize_cycle_report(self, cycle_report: TDDCycleReport) -> Dict:
        """ã‚µã‚¤ã‚¯ãƒ«ãƒ¬ãƒãƒ¼ãƒˆã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º"""
        
        def serialize_phase(phase: TDDPhaseResult) -> Dict:
            return {
                "phase_name": phase.phase_name,
                "success": phase.success,
                "execution_time_seconds": phase.execution_time_seconds,
                "tests_passed": phase.tests_passed,
                "tests_failed": phase.tests_failed,
                "coverage_percentage": phase.coverage_percentage,
                "performance_metrics": phase.performance_metrics,
                "error_messages": phase.error_messages,
                "quality_indicators": phase.quality_indicators
            }
        
        return {
            "cycle_id": cycle_report.cycle_id,
            "timestamp": cycle_report.timestamp.isoformat(),
            "red_phase": serialize_phase(cycle_report.red_phase),
            "green_phase": serialize_phase(cycle_report.green_phase),
            "refactor_phase": serialize_phase(cycle_report.refactor_phase),
            "overall_success": cycle_report.overall_success,
            "quality_score": cycle_report.quality_score,
            "recommendations": cycle_report.recommendations
        }
    
    def _generate_readable_report(self, cycle_report: TDDCycleReport) -> str:
        """äººé–“å¯èª­ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        
        report_lines = [
            f"# TDD Cycle Report: {cycle_report.cycle_id}",
            f"**Generated:** {cycle_report.timestamp.strftime('%Y-%m-%d %H:%M:%S')}",
            f"**Overall Success:** {'âœ… PASS' if cycle_report.overall_success else 'âŒ FAIL'}",
            f"**Quality Score:** {cycle_report.quality_score:.3f}/1.000",
            "",
            "## Phase Results",
            ""
        ]
        
        phases = [
            ("ğŸ”´ Red Phase", cycle_report.red_phase),
            ("ğŸŸ¢ Green Phase", cycle_report.green_phase), 
            ("ğŸ”§ Refactor Phase", cycle_report.refactor_phase)
        ]
        
        for phase_name, phase_result in phases:
            success_icon = "âœ…" if phase_result.success else "âŒ"
            report_lines.extend([
                f"### {phase_name} {success_icon}",
                f"- **Execution Time:** {phase_result.execution_time_seconds:.2f} seconds",
                f"- **Tests Passed:** {phase_result.tests_passed}",
                f"- **Tests Failed:** {phase_result.tests_failed}",
                f"- **Coverage:** {phase_result.coverage_percentage:.1f}%",
                ""
            ])
            
            if phase_result.performance_metrics:
                report_lines.append("**Performance Metrics:**")
                for metric, value in phase_result.performance_metrics.items():
                    report_lines.append(f"- {metric}: {value:.2f}")
                report_lines.append("")
            
            if phase_result.error_messages:
                report_lines.extend([
                    "**Errors:**",
                    *[f"- {error}" for error in phase_result.error_messages],
                    ""
                ])
        
        report_lines.extend([
            "## Recommendations",
            "",
            *[f"- {rec}" for rec in cycle_report.recommendations],
            "",
            "---",
            "*Generated by TDD Execution Orchestrator*"
        ])
        
        return "\n".join(report_lines)
    
    def generate_comprehensive_summary(self) -> Dict[str, Any]:
        """åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ"""
        
        if not self.execution_history:
            return {"message": "No TDD cycles executed yet"}
        
        # çµ±è¨ˆè¨ˆç®—
        total_cycles = len(self.execution_history)
        successful_cycles = sum(1 for cycle in self.execution_history if cycle.overall_success)
        success_rate = successful_cycles / total_cycles
        
        quality_scores = [cycle.quality_score for cycle in self.execution_history]
        avg_quality = statistics.mean(quality_scores)
        
        # æœ€æ–°ã‚µã‚¤ã‚¯ãƒ«ã®åˆ†æ
        latest_cycle = self.execution_history[-1]
        
        return {
            "execution_summary": {
                "total_cycles": total_cycles,
                "successful_cycles": successful_cycles,
                "success_rate": success_rate,
                "average_quality_score": avg_quality,
                "latest_cycle_id": latest_cycle.cycle_id
            },
            "quality_trends": {
                "quality_scores": quality_scores,
                "improving": len(quality_scores) > 1 and quality_scores[-1] > quality_scores[-2],
                "meets_standards": avg_quality >= self.performance_benchmarks["min_quality_score"]
            },
            "performance_benchmarks": self.performance_benchmarks,
            "recommendations": self._generate_comprehensive_recommendations()
        }
    
    def _generate_comprehensive_recommendations(self) -> List[str]:
        """åŒ…æ‹¬çš„æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        
        if not self.execution_history:
            return ["Execute TDD cycles to generate recommendations"]
        
        recommendations = []
        
        # æˆåŠŸç‡åˆ†æ
        success_rate = sum(1 for cycle in self.execution_history if cycle.overall_success) / len(self.execution_history)
        if success_rate < 0.8:
            recommendations.append("Focus on improving TDD cycle success rate - aim for >80%")
        
        # å“è³ªã‚¹ã‚³ã‚¢åˆ†æ
        avg_quality = statistics.mean([cycle.quality_score for cycle in self.execution_history])
        if avg_quality < self.performance_benchmarks["min_quality_score"]:
            recommendations.append("Improve overall quality score to meet minimum standards")
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        if len(self.execution_history) > 2:
            recent_quality = statistics.mean([cycle.quality_score for cycle in self.execution_history[-3:]])
            earlier_quality = statistics.mean([cycle.quality_score for cycle in self.execution_history[:-3]])
            
            if recent_quality < earlier_quality:
                recommendations.append("Quality trend declining - review recent changes")
        
        if not recommendations:
            recommendations.append("Excellent TDD implementation - maintain current standards")
        
        return recommendations


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    
    print("ğŸ¯ TDD Execution Orchestrator - çµ±åˆæƒ…å ±ã‚·ã‚¹ãƒ†ãƒ å­˜åœ¨è«–çš„çµ‚äº†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£")
    print("=" * 80)
    print("ğŸ”¬ æ­¦ç”°ç«¹å¤«ï¼ˆt_wadaï¼‰TDDå°‚é–€çŸ¥è­˜ã«åŸºã¥ãå“è³ªä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ ")
    print("ğŸ“Š Red-Green-Refactorã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œãƒ»åˆ†æãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
    print("=" * 80)
    
    # TDDã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ã®åˆæœŸåŒ–
    orchestrator = TDDExecutionOrchestrator()
    
    try:
        # å®Œå…¨ãªTDDã‚µã‚¤ã‚¯ãƒ«ã®å®Ÿè¡Œ
        cycle_report = await orchestrator.execute_complete_tdd_cycle()
        
        print(f"\nğŸ“‹ TDD Cycle Results: {cycle_report.cycle_id}")
        print("-" * 50)
        print(f"Overall Success: {'âœ… PASS' if cycle_report.overall_success else 'âŒ FAIL'}")
        print(f"Quality Score: {cycle_report.quality_score:.3f}/1.000")
        
        # ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥çµæœ
        phases = [
            ("Red Phase", cycle_report.red_phase),
            ("Green Phase", cycle_report.green_phase),
            ("Refactor Phase", cycle_report.refactor_phase)
        ]
        
        for phase_name, phase_result in phases:
            status = "âœ… PASS" if phase_result.success else "âŒ FAIL"
            print(f"{phase_name}: {status} ({phase_result.tests_passed}P/{phase_result.tests_failed}F, {phase_result.coverage_percentage:.1f}% coverage)")
        
        # æ¨å¥¨äº‹é …
        if cycle_report.recommendations:
            print(f"\nğŸ’¡ Recommendations:")
            for i, rec in enumerate(cycle_report.recommendations, 1):
                print(f"   {i}. {rec}")
        
        # åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ
        summary = orchestrator.generate_comprehensive_summary()
        print(f"\nğŸ“Š Comprehensive Summary:")
        print(f"   Success Rate: {summary['execution_summary']['success_rate']:.1%}")
        print(f"   Average Quality: {summary['execution_summary']['average_quality_score']:.3f}")
        print(f"   Meets Standards: {'âœ…' if summary['quality_trends']['meets_standards'] else 'âŒ'}")
        
        # æœ€çµ‚è©•ä¾¡
        print(f"\n" + "=" * 80)
        if cycle_report.overall_success and cycle_report.quality_score >= 0.9:
            print("ğŸ‰ TDD SUCCESS: Production deployment criteria satisfied!")
            print("âœ¨ Architecture implementation meets all quality standards")
        else:
            print("âš ï¸  TDD REVIEW REQUIRED: Implementation needs improvement")
            print("ğŸ”§ Address recommendations before production deployment")
        print("=" * 80)
        
        return 0 if cycle_report.overall_success else 1
        
    except Exception as e:
        logger.error(f"TDD execution failed: {e}")
        print(f"âŒ TDD Execution Error: {e}")
        return 1


if __name__ == "__main__":
    # éåŒæœŸå®Ÿè¡Œ
    exit_code = asyncio.run(main())
    sys.exit(exit_code)